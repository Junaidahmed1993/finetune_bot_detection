'''1. Generating the WordClouds'''
'''1.1. Word Clouds on Bot Data'''
# We have to repeat following code for each of the dataset
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Read the CSV file into a DataFrame
df = pd.read_csv('anti_env_corpus1.csv')

# Filter the DataFrame to only include data generated by bots
bot_data = df[df['classification'] == 'bot']

# Fill NaN values in the 'cleaned' column with empty strings
bot_data['cleaned'].fillna('', inplace=True)

# Combine all the text from the 'cleaned' column into a single string
text = ' '.join(bot_data['cleaned'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Anti-Vaccine Datasets')
plt.show()


'''1.2 WordClouds-Overall Data'''
# Following code will generate the wordcloud based upon entire data
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Read the CSV file into a DataFrame
df = pd.read_csv('anti_env_corpus1.csv')

# Fill NaN values in the 'cleaned' column with empty strings
df['cleaned'].fillna('', inplace=True)

# Combine all the text from the 'cleaned' column into a single string
text = ' '.join(df['cleaned'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Anti-Vaccine Datasets')
plt.show()


'''2. Top words'''
'''2.1. Top words in All of Datasets'''
import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function to get top words
def get_top_words(text, num_words=10):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]
    word_counts = Counter(filtered_words)
    return word_counts.most_common(num_words)

# Read CSV files
df1 = pd.read_csv('anti_env_corpus1.csv')
df2 = pd.read_csv('anti_vaccine_corpus1.csv')
df3 = pd.read_csv('pro_env_corpus.csv')
df4 = pd.read_csv('pro_vacc_corpus.csv')

# Combine text for each dataset
text1 = ' '.join(df1['cleaned'].dropna())
text2 = ' '.join(df2['cleaned'].dropna())
text3 = ' '.join(df3['cleaned'].dropna())
text4 = ' '.join(df4['cleaned'].dropna())

# Get top words for each dataset
top_words1 = get_top_words(text1)
top_words2 = get_top_words(text2)
top_words3 = get_top_words(text3)
top_words4 = get_top_words(text4)

# Create subplots with reduced vertical spacing
fig = make_subplots(rows=2, cols=2, subplot_titles=('Anti-Environment', 'Anti-Vaccine', 'Pro-Environment', 'Pro-Vaccine'), vertical_spacing=0.1)

# Add bar charts to subplots
def add_trace(fig, top_words, row, col):
    words, counts = zip(*top_words)
    trace = go.Bar(x=counts, y=words, orientation='h', showlegend=False)
    fig.add_trace(trace, row=row, col=col)

add_trace(fig, top_words1, 1, 1)
add_trace(fig, top_words2, 1, 2)
add_trace(fig, top_words3, 2, 1)
add_trace(fig, top_words4, 2, 2)

# Update layout
fig.update_layout(height=800, width=1000, title_text="Top Words in Datasets", showlegend=False)
fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))
fig.update_yaxes(title_text='Words')
fig.update_xaxes(title_text='Counts')
fig.update_layout(title_x=0.5)
fig.update_layout(autosize=True)

fig.show()

'''2.1. Top Words in Bot Datasets'''
import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function to get top words
def get_top_words(text, num_words=10):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]
    word_counts = Counter(filtered_words)
    return word_counts.most_common(num_words)

# Read CSV files
df1 = pd.read_csv('anti_env_corpus1.csv')
df2 = pd.read_csv('anti_vaccine_corpus1.csv')
df3 = pd.read_csv('pro_env_corpus.csv')
df4 = pd.read_csv('pro_vacc_corpus.csv')

# Filter DataFrames to only include data generated by bots
bot_data1 = df1[df1['classification'] == 'bot']
bot_data2 = df2[df2['classification'] == 'bot']
bot_data3 = df3[df3['classification'] == 'bot']
bot_data4 = df4[df4['classification'] == 'bot']

# Combine text for each bot dataset
text1 = ' '.join(bot_data1['cleaned'].dropna())
text2 = ' '.join(bot_data2['cleaned'].dropna())
text3 = ' '.join(bot_data3['cleaned'].dropna())
text4 = ' '.join(bot_data4['cleaned'].dropna())

# Get top words for each dataset
top_words1 = get_top_words(text1)
top_words2 = get_top_words(text2)
top_words3 = get_top_words(text3)
top_words4 = get_top_words(text4)

# Create subplots with reduced vertical spacing
fig = make_subplots(rows=2, cols=2, subplot_titles=('Anti-Environment Bots', 'Anti-Vaccine Bots', 'Pro-Environment Bots', 'Pro-Vaccine Bots'), vertical_spacing=0.1)

# Add bar charts to subplots
def add_trace(fig, top_words, row, col):
    words, counts = zip(*top_words)
    trace = go.Bar(x=counts, y=words, orientation='h', showlegend=False)
    fig.add_trace(trace, row=row, col=col)

add_trace(fig, top_words1, 1, 1)
add_trace(fig, top_words2, 1, 2)
add_trace(fig, top_words3, 2, 1)
add_trace(fig, top_words4, 2, 2)

# Update layout
fig.update_layout(height=800, width=1000, title_text="Top Words in Bot Datasets", showlegend=False)
fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))
fig.update_yaxes(title_text='Words')
fig.update_xaxes(title_text='Counts')
fig.update_layout(title_x=0.5)
fig.update_layout(autosize=True)

fig.show()


'''2.3. Bigrams=Overall datasets'''
import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.util import ngrams
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function to get top bigrams
def get_top_bigrams(text, num_bigrams=10):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]
    bigrams = ngrams(filtered_words, 2)
    bigram_counts = Counter(bigrams)
    return bigram_counts.most_common(num_bigrams)

# Read CSV files
df1 = pd.read_csv('anti_env_corpus1.csv')
df2 = pd.read_csv('anti_vaccine_corpus1.csv')
df3 = pd.read_csv('pro_env_corpus.csv')
df4 = pd.read_csv('pro_vacc_corpus.csv')

# Combine text for each dataset
text1 = ' '.join(df1['cleaned'].dropna())
text2 = ' '.join(df2['cleaned'].dropna())
text3 = ' '.join(df3['cleaned'].dropna())
text4 = ' '.join(df4['cleaned'].dropna())

# Get top bigrams for each dataset
top_bigrams1 = get_top_bigrams(text1)
top_bigrams2 = get_top_bigrams(text2)
top_bigrams3 = get_top_bigrams(text3)
top_bigrams4 = get_top_bigrams(text4)

# Create subplots with increased horizontal spacing
fig_bigrams = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Anti-Environment Bigrams', 'Anti-Vaccine Bigrams', 'Pro-Environment Bigrams', 'Pro-Vaccine Bigrams'),
    vertical_spacing=0.1,
    horizontal_spacing=0.3
)

# Add bar charts to subplots
def add_trace(fig, top_items, row, col):
    items, counts = zip(*top_items)
    labels = [' '.join(item) for item in items]
    trace = go.Bar(x=counts, y=labels, orientation='h', showlegend=False)
    fig.add_trace(trace, row=row, col=col)

# Add traces for bigrams
add_trace(fig_bigrams, top_bigrams1, 1, 1)
add_trace(fig_bigrams, top_bigrams2, 1, 2)
add_trace(fig_bigrams, top_bigrams3, 2, 1)
add_trace(fig_bigrams, top_bigrams4, 2, 2)

# Update layout
fig_bigrams.update_layout(height=800, width=1000, title_text="Top Bigrams in Datasets", showlegend=False)
fig_bigrams.update_layout(margin=dict(l=20, r=20, t=50, b=20))
fig_bigrams.update_yaxes(title_text='Bigrams')
fig_bigrams.update_xaxes(title_text='Counts')
fig_bigrams.update_layout(title_x=0.5)
fig_bigrams.update_layout(autosize=True)

fig_bigrams.show()

'''2.4. Biagrams=Bot Datasets'''
import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.util import ngrams
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function to get top bigrams
def get_top_bigrams(text, num_bigrams=10):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]
    bigrams = ngrams(filtered_words, 2)
    bigram_counts = Counter(bigrams)
    return bigram_counts.most_common(num_bigrams)

# Read CSV files
df1 = pd.read_csv('anti_env_corpus1.csv')
df2 = pd.read_csv('anti_vaccine_corpus1.csv')
df3 = pd.read_csv('pro_env_corpus.csv')
df4 = pd.read_csv('pro_vacc_corpus.csv')

# Filter DataFrames to only include data generated by bots
bot_data1 = df1[df1['classification'] == 'bot']
bot_data2 = df2[df2['classification'] == 'bot']
bot_data3 = df3[df3['classification'] == 'bot']
bot_data4 = df4[df4['classification'] == 'bot']

# Combine text for each bot dataset
text1 = ' '.join(bot_data1['cleaned'].dropna())
text2 = ' '.join(bot_data2['cleaned'].dropna())
text3 = ' '.join(bot_data3['cleaned'].dropna())
text4 = ' '.join(bot_data4['cleaned'].dropna())

# Get top bigrams for each dataset
top_bigrams1 = get_top_bigrams(text1)
top_bigrams2 = get_top_bigrams(text2)
top_bigrams3 = get_top_bigrams(text3)
top_bigrams4 = get_top_bigrams(text4)

# Create subplots with increased horizontal spacing
fig_bigrams = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Anti-Environment Bots Bigrams', 'Anti-Vaccine Bots Bigrams', 'Pro-Environment Bots Bigrams', 'Pro-Vaccine Bots Bigrams'),
    vertical_spacing=0.1,
    horizontal_spacing=0.3
)

# Add bar charts to subplots
def add_trace(fig, top_items, row, col):
    items, counts = zip(*top_items)
    labels = [' '.join(item) for item in items]
    trace = go.Bar(x=counts, y=labels, orientation='h', showlegend=False)
    fig.add_trace(trace, row=row, col=col)

# Add traces for bigrams
add_trace(fig_bigrams, top_bigrams1, 1, 1)
add_trace(fig_bigrams, top_bigrams2, 1, 2)
add_trace(fig_bigrams, top_bigrams3, 2, 1)
add_trace(fig_bigrams, top_bigrams4, 2, 2)

# Update layout
fig_bigrams.update_layout(height=800, width=1000, title_text="Top Bigrams in Bot Datasets", showlegend=False)
fig_bigrams.update_layout(margin=dict(l=20, r=20, t=50, b=20))
fig_bigrams.update_yaxes(title_text='Bigrams')
fig_bigrams.update_xaxes(title_text='Counts')
fig_bigrams.update_layout(title_x=0.5)
fig_bigrams.update_layout(autosize=True)

fig_bigrams.show()

'''2.5. Top Triagram in Overall Datasets'''
import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.util import ngrams
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function to get top trigrams
def get_top_trigrams(text, num_trigrams=10):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]
    trigrams = ngrams(filtered_words, 3)
    trigram_counts = Counter(trigrams)
    return trigram_counts.most_common(num_trigrams)

# Read CSV files
df1 = pd.read_csv('anti_env_corpus1.csv')
df2 = pd.read_csv('anti_vaccine_corpus1.csv')
df3 = pd.read_csv('pro_env_corpus.csv')
df4 = pd.read_csv('pro_vacc_corpus.csv')

# Combine text for each dataset
text1 = ' '.join(df1['cleaned'].dropna())
text2 = ' '.join(df2['cleaned'].dropna())
text3 = ' '.join(df3['cleaned'].dropna())
text4 = ' '.join(df4['cleaned'].dropna())

# Get top trigrams for each dataset
top_trigrams1 = get_top_trigrams(text1)
top_trigrams2 = get_top_trigrams(text2)
top_trigrams3 = get_top_trigrams(text3)
top_trigrams4 = get_top_trigrams(text4)

# Create subplots with increased horizontal spacing and reduced vertical spacing
fig_trigrams = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Anti-Environment Trigrams', 'Anti-Vaccine Trigrams', 'Pro-Environment Trigrams', 'Pro-Vaccine Trigrams'),
    vertical_spacing=0.2,
    horizontal_spacing=0.25
)

# Add bar charts to subplots
def add_trace(fig, top_items, row, col):
    items, counts = zip(*top_items)
    labels = [' '.join(item) for item in items]
    trace = go.Bar(x=counts, y=labels, orientation='h', showlegend=False, textfont=dict(size=10))
    fig.add_trace(trace, row=row, col=col)

# Add traces for trigrams
add_trace(fig_trigrams, top_trigrams1, 1, 1)
add_trace(fig_trigrams, top_trigrams2, 1, 2)
add_trace(fig_trigrams, top_trigrams3, 2, 1)
add_trace(fig_trigrams, top_trigrams4, 2, 2)

# Update layout with increased width and font size adjustments
fig_trigrams.update_layout(height=800, width=1400, title_text="Top Trigrams in Datasets", showlegend=False)
fig_trigrams.update_layout(margin=dict(l=20, r=20, t=50, b=20))
fig_trigrams.update_yaxes(title_text='Trigrams', tickfont=dict(size=10))
fig_trigrams.update_xaxes(title_text='Counts')
fig_trigrams.update_layout(title_x=0.5)
fig_trigrams.update_layout(autosize=True)

fig_trigrams.show()

'''2.6. Top Triagrams in Bot Datasets'''
import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.util import ngrams
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function to get top trigrams
def get_top_trigrams(text, num_trigrams=10):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]
    trigrams = ngrams(filtered_words, 3)
    trigram_counts = Counter(trigrams)
    return trigram_counts.most_common(num_trigrams)

# Read CSV files
df1 = pd.read_csv('anti_env_corpus1.csv')
df2 = pd.read_csv('anti_vaccine_corpus1.csv')
df3 = pd.read_csv('pro_env_corpus.csv')
df4 = pd.read_csv('pro_vacc_corpus.csv')

# Filter DataFrames to only include data generated by bots
bot_data1 = df1[df1['classification'] == 'bot']
bot_data2 = df2[df2['classification'] == 'bot']
bot_data3 = df3[df3['classification'] == 'bot']
bot_data4 = df4[df4['classification'] == 'bot']

# Combine text for each bot dataset
text1 = ' '.join(bot_data1['cleaned'].dropna())
text2 = ' '.join(bot_data2['cleaned'].dropna())
text3 = ' '.join(bot_data3['cleaned'].dropna())
text4 = ' '.join(bot_data4['cleaned'].dropna())

# Get top trigrams for each dataset
top_trigrams1 = get_top_trigrams(text1)
top_trigrams2 = get_top_trigrams(text2)
top_trigrams3 = get_top_trigrams(text3)
top_trigrams4 = get_top_trigrams(text4)

# Create subplots with increased horizontal spacing and reduced vertical spacing
fig_trigrams_bots = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Anti-Environment Bots Trigrams', 'Anti-Vaccine Bots Trigrams', 'Pro-Environment Bots Trigrams', 'Pro-Vaccine Bots Trigrams'),
    vertical_spacing=0.2,
    horizontal_spacing=0.25
)

# Add bar charts to subplots
def add_trace(fig, top_items, row, col):
    items, counts = zip(*top_items)
    labels = [' '.join(item) for item in items]
    trace = go.Bar(x=counts, y=labels, orientation='h', showlegend=False, textfont=dict(size=10))
    fig.add_trace(trace, row=row, col=col)

# Add traces for trigrams
add_trace(fig_trigrams_bots, top_trigrams1, 1, 1)
add_trace(fig_trigrams_bots, top_trigrams2, 1, 2)
add_trace(fig_trigrams_bots, top_trigrams3, 2, 1)
add_trace(fig_trigrams_bots, top_trigrams4, 2, 2)

# Update layout with increased width and font size adjustments
fig_trigrams_bots.update_layout(height=800, width=1400, title_text="Top Trigrams in Bot Datasets", showlegend=False)
fig_trigrams_bots.update_layout(margin=dict(l=20, r=20, t=50, b=20))
fig_trigrams_bots.update_yaxes(title_text='Trigrams', tickfont=dict(size=10))
fig_trigrams_bots.update_xaxes(title_text='Counts')
fig_trigrams_bots.update_layout(title_x=0.5)
fig_trigrams_bots.update_layout(autosize=True)

fig_trigrams_bots.show()

'''2.7. Topic Modelling'''
# Note, we will repeat following code for the each of our datasets

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.corpora import Dictionary
from gensim.models import LdaModel
from pprint import pprint

df= pd.read_csv('anti_env_corpus1.csv')

# Download NLTK resources
nltk.download('stopwords')
nltk.download('punkt')


# Filter bot data
bot_data = df[df['classification'] == 'bot']

# Remove stopwords and handle NaN values appropriately
stop_words = set(stopwords.words('english'))
bot_data['cleaned_no_stopwords'] = bot_data['cleaned'].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in stop_words and pd.notnull(word)]))

# Tokenization
bot_data['tokenized'] = bot_data['cleaned_no_stopwords'].apply(lambda x: word_tokenize(x.lower()))

# Create dictionary and corpus
dictionary = Dictionary(bot_data['tokenized'])
corpus = [dictionary.doc2bow(doc) for doc in bot_data['tokenized']]

# Train LDA model
lda_model = LdaModel(corpus=corpus,
                     id2word=dictionary,
                     num_topics=5,  # Specify the number of topics
                     passes=10,     # Number of passes through the corpus
                     iterations=100,  # Maximum number of iterations through the corpus
                     random_state=42)

# Print topics
print("Topics:")
pprint(lda_model.print_topics())

# Visualize topics
import pyLDAvis.gensim
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)
pyLDAvis.save_html(vis, 'lda_visualization_vacc_datasets.html')  # Save visualization to HTML file

